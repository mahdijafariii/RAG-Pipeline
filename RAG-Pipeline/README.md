# RAG-Pipeline Backend

This is the backend of the RAG (Retrieval-Augmented Generation) pipeline, built using **FastAPI**. It combines information retrieval with large language models (LLMs) to provide high-quality, context-aware answers from documents or a custom knowledge base.

## ğŸ”§ Features

- âœ… FastAPI-powered RESTful API
- ğŸ“„ Document chunking and semantic indexing
- ğŸ” Retrieval system for fetching relevant context
- ğŸ¤– Pluggable LLM interfaces (OpenAI, Hugging Face, Gemini, etc.)
- ğŸ” Modular architecture with interchangeable components
- ğŸ§ª Easily testable and extendable

---

## ğŸ§± Project Structure

RAG-Pipeline/
â”œâ”€â”€ generation/
â”‚ â”œâ”€â”€ init.py
â”‚ â”œâ”€â”€ generation.py # Main generation manager
â”‚ â”œâ”€â”€ huggingface.py # Hugging Face LLM integration
â”‚ â”œâ”€â”€ gemeni.py # Gemini (Google) LLM integration
â”‚ â”œâ”€â”€ together.py # Together.ai API wrapper
â”‚ â”œâ”€â”€ ollama.py # Local LLM support via Ollama
â”œâ”€â”€ retrieval.py # Context retrieval from indexed documents
â”œâ”€â”€ main.py # FastAPI app entry point
â”œâ”€â”€ requirements.txt # Project dependencies


---

## ğŸ§  How it Works

1. **Document Embedding**: Input documents are chunked and vectorized using embedding models.
2. **Context Retrieval**: At query time, relevant chunks are retrieved using similarity search.
3. **Generation**: A selected LLM is used to generate an answer based on the retrieved context.
4. **Response**: The system returns the context-aware answer to the client.

---

## ğŸš€ Getting Started

### 1. Clone the Repository

```bash
git clone https://github.com/YourUsername/RAG-Pipeline.git
cd RAG-Pipeline
python -m venv .venv
source .venv/bin/activate   # On Windows: .venv\Scripts\activate
pip install -r requirements.txt
uvicorn main:app --reload
```

âœ… Supported LLM Engines
- OpenAI (gpt-3.5, gpt-4)
- HuggingFace models (e.g., bert-base, flan-t5)
- Google Gemini
- Together AI
- Ollama (local deployment)


